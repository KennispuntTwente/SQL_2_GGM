# Dit is een voorbeeld van een .env-configuratiebestand voor de 'source_to_staging' module
# Kopieer dit bestand naar '.env' en pas de waarden aan naar jouw omgeving

# [database-source]
SRC_DRIVER = oracle+oracledb
SRC_HOST = localhost
SRC_PORT = 1521
SRC_USERNAME = sa
SRC_PASSWORD = SecureP@ss1!24323482349
SRC_DB = ggm
SRC_SCHEMA = 
# (Optioneel) Oracle Instant Client-pad voor thick-mode (bijv. "C:\\oracle\\instantclient_21_18")
SRC_ORACLE_CLIENT_PATH =
# (Optioneel) Gebruik je een Oracle TNS‑alias? Zet dan op True en geef de alias op via SRC_HOST of SRC_DB
SRC_ORACLE_TNS_ALIAS = False
# Optioneel (alleen voor Microsoft SQL Server via pyodbc): kies de ODBC-driver
# Mogelijke waarden: "ODBC Driver 18 for SQL Server", "ODBC Driver 17 for SQL Server", "SQL Server"
# Laat leeg om standaard (18) te gebruiken
SRC_MSSQL_ODBC_DRIVER = 

# [database-destination]
DST_DRIVER = postgresql+psycopg2
DST_HOST = localhost
DST_PORT = 5432
DST_USERNAME = sa
DST_PASSWORD = SecureP@ss1!24323482349
DST_DB = ggm
DST_SCHEMA = ggm
# (Optioneel) Oracle Instant Client-pad voor thick-mode (bijv. "C:\\oracle\\instantclient_21_18")
DST_ORACLE_CLIENT_PATH =
# (Optioneel) Gebruik je een Oracle TNS‑alias? Zet dan op True en geef de alias op via DST_HOST of DST_DB
DST_ORACLE_TNS_ALIAS = False
# Optioneel (alleen voor Microsoft SQL Server via pyodbc): kies de ODBC-driver
# Mogelijke waarden: "ODBC Driver 18 for SQL Server", "ODBC Driver 17 for SQL Server", "SQL Server"
# Laat leeg om standaard (18) te gebruiken
DST_MSSQL_ODBC_DRIVER = 

# [settings]
# Comma-separated van tabellen om op te halen uit de brondatabase ('database-source')
SRC_TABLES = szclient, wvbedrag, wvaanb

# Chunk size; aantal rijen dat in één keer in werkgeheugen wordt geladen
# Dit wordt gebruikt om problemen met 'larger-than-memory' data op te lossen
# Je kan het kleiner of groter maken afhankelijk van hoeveel werkgeheugen (RAM) je machine heeft
SRC_CHUNK_SIZE = 100000

# Kies de gewenste transfer modus (één van):
#  - SQLALCHEMY_DIRECT: lees via SQLAlchemy naar werkgeheugen, upload direct
#  - SQLALCHEMY_DUMP: lees via SQLAlchemy en dump naar Parquet, upload daarna Parquet
#  - CONNECTORX_DUMP: lees via ConnectorX en dump naar Parquet, upload daarna Parquet
TRANSFER_MODE = SQLALCHEMY_DIRECT

# Of wachtwoord gevraagd wordt in de console i.p.v. uit te lezen uit je environment variables
# of uit je .ini-config. Als je hiervan gebruikt maakt betekent het ook dat het script
# alleen interactief gerund kan worden
ASK_PASSWORD_IN_CLI = False

# (Optioneel) Direct transfer transient error retries (alleen SQLALCHEMY_DIRECT)
# Aantal keer dat een batch-insert opnieuw geprobeerd wordt bij tijdelijke fouten
# (deadlock, timeout, disconnect). Defaults: 3; 0 om retries uit te schakelen.
DIRECT_MAX_RETRIES = 3
# Start-backoff in seconden voor retries (exponential met jitter). Default: 0.5
DIRECT_BACKOFF_BASE_SECONDS = 0.5
# Max-backoff in seconden voor retries. Default: 8.0
DIRECT_BACKOFF_MAX_SECONDS = 8.0

# Of de gedownloadde parquet-files na het uploaden naar 'database-destination' moeten
# worden verwijderd van de schijfruimte van de machine waar de Python-code draait
CLEANUP_PARQUET_FILES = True

# [logging]
# Globale logging-niveau; vanaf welk type message moet gelogd worden?
# Kan zijn: DEBUG, INFO, WARNING, of ERROR
LOG_LEVEL = INFO
# Of logs in een logbestand moeten worden opgeslagen
LOG_TO_FILE = True
# Locatie van het logbestand (wordt aangemaakt indien deze nog niet bestaat)
LOG_FILE = logs/source_to_staging.log
# Max. grootte (in bytes) van het logbestand voordat er een nieuw bestand wordt gestart (rotatie). Voorbeeld: 5.000.000 ≈ 5 MB
LOG_ROTATE_BYTES = 5000000
# Aantal oude logbestanden dat bewaard blijft bij rotatie (bijv. app.log.1 t/m app.log.N)
LOG_BACKUP_COUNT = 3
# Optioneel: eigen opmaak van logregels. Laat weg voor standaard; voorbeeld toont tijd, level, logger en bericht
# LOG_FORMAT = %(asctime)s %(levelname)-8s [%(name)s] %(message)s
